<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>

    <title>未知</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="../stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="../page_styles.css" rel="stylesheet" type="text/css"/>

  


<link href="../calibreHtmlOutBasicCss.css" type="text/css" rel="stylesheet" />

</head>
<body>

<div class="calibreMeta">
  <div class="calibreMetaTitle">
  
  
    
    <h1>
      <a href="../../jhnii3.html">Kafka入门与实践
</a>
    </h1>
    
    
  
  </div>
  <div class="calibreMetaAuthor">
    牟大恩

  </div>
</div>

<div class="calibreMain">

  <div class="calibreEbookContent">
    
      <div class="calibreEbNavTop">
        
          <a href="part0009_split_003.html" class="calibreAPrev">previous page</a>
        

        
          <a href="part0009_split_005.html" class="calibreANext"> next page</a>
        
      </div>
    

    
<h2 id="nav_point_152" class="sigil_not_in_toc">6.4　自定义组件实现</h2>

  <p class="zw">Kafka对部分配置的属性值提供了统一接口，允许用户自定义其实现，客户端只需要实现该接口相应方法，在方法中根据业务需要进行定制，例如，客户端可以自定义分区器以及序列化与反序列化类。本节结合实例简单介绍客户端自定义相关类的具体用法。</p>

  <h3 id="nav_point_153" class="calibre9">6.4.1　分区器</h3>

  <p class="zw">在实际应用中，有可能Kafka默认分区策略并不能很好地满足业务需要，此时就需要根据Kafka提供的API开发定制满足业务场景的分区策略，也就是需要自定义一个分区器。自定义一个分区器的基本流程如下。</p>

  <p class="zw">（1）实现org.apache.kafka.clients.producer.Partitioner接口，重写该接口的int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster)方法，在该方法中实现分区分配的算法。</p>

  <p class="zw">（2）在实例化KafkaProducer的配置中指定partitioner.class为自定义的分区器。</p>

  <p class="zw">继续本章中向Kafka发送股票行情的实例，自定义一个股票行情相关的分区器，该分区器根据股票代码最后一位数字与分区总数取模的策略来分配消息。其实可以根据股票代码对应的市场来划分，这样同一个市场的股票就会在同一个分区，这里只是介绍API的用法并不太关注业务本身。</p>

  <p class="zw">为实现该功能，我们首先自定义一个名为StockPartitioner的分区器，该类实现了org.apache. kafka.clients.producer.Partitioner接口，同时重写分区分配方法。自定义分区器实现逻辑详见代码清单6-23所示。</p>

  <p class="calibre21"><strong class="calibre1">代码清单6-23　自定义分区器的实现逻辑</strong></p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">package com.kafka.action.chapter6.producer;

import Java.util.Map;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.log4j.Logger;
public class StockPartitionor implements Partitioner {
    private static final Logger LOG = Logger.getLogger(StockPartitionor.class);
    /** 分区数 */
    private static final Integer PARTITIONS = 6;
    @Override
    public void configure(Map&lt;String, ?&gt; arg0) {
    }
    @Override
    public void close() {
    }
    /**
     * 根据股票代码最后一位与分区总长度取模来作为分区分配的策略
     */
    @Override
    public int partition(String topic, Object key, byte[] keyBytes,
    Object value, byte[] valueBytes, Cluster cluster) {
        if (null == key) {
            return 0;
        }
        String stockCode = String.valueOf(key);
        try {
            int partitionId = Integer.valueOf(stockCode.substring(stockCode
                                              .length() - 2)) % PARTITIONS;
            return partitionId;
        } catch (NumberFormatException e) {
            LOG.error("Parse message key occurs exception,key:" + stockCode, e);
            return 0;
        }
    }
}</code></pre>

  <p class="zw">然后修改QuotationProducer.initConfig()方法，在Properties中增加patitioner.class设置，代码如下：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, 
               StockPartitionor.class. getName());</code></pre>

  <p class="zw">为了便于验证分区策略，我们新创建一个主题，命令如下：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">./kafka-topics.sh --create --zookeeper server-1:2181,server-2:2181,server-3:2181 --replication-factor 1 --partitions 6 --topic stock-quotation-partition</code></pre>

  <p class="zw">最后修改行情推送生产者相应逻辑，向主题stock-quotation-partition发送一批行情信息，待程序执行结束之后，执行以下命令查看stock-quotation-partition-3分区中的消息：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">kafka-run-class.sh kafka.tools.DumpLogSegments --files /opt/data/kafka-logs/stock- quotation-partition-3/00000000000000000000.log --print-data-log</code></pre>

  <p class="zw">输出信息部分内容如下：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">offset: 0 position: 0 CreateTime: 1488453153954 isvalid: true payloadsize: 44 magic: 1 compresscodec: NoCompressionCodec crc: 4029733061 keysize: 6 key: 600109 payload: 600109|股票-600109|1488453153954|11.8|10.5
offset: 1 position: 84 CreateTime: 1488453153956 isvalid: true payloadsize: 45 magic: 1 compresscodec: NoCompressionCodec crc: 2342800744 keysize: 6 key: 600103 payload: 600103|股票-600103|1488453153956|11.8|10.75
offset: 2 position: 169 CreateTime: 1488453153956 isvalid: true payloadsize: 45 magic: 1 compresscodec: NoCompressionCodec crc: 2816793266 keysize: 6 key: 600103 payload: 600103|股票-600103|1488453153956|11.8|10.41
offset: 3 position: 254 CreateTime: 1488453155982 isvalid: true payloadsize: 45 magic: 1 compresscodec: NoCompressionCodec crc: 3879419630 keysize: 6 key: 600109 payload: 600109|股票-600109|1488453155982|11.8|10.44</code></pre>

  <p class="zw">从输出结果可知：分配到该分区的所有数据都满足消息的Key最后一位数字与分区总数6取模的值等于分区编号3，达到了预期目标。</p>

  <h3 id="nav_point_154" class="calibre9">6.4.2　序列化与反序列化</h3>

  <p class="zw">Kafka对外提供了统一的序列化与反序列化接口，客户端通过实现这两个接口自定义序列化与反序列化类。本小节介绍如何利用Avro序列化框架来自定义序列化与反序列化类。</p>

  <p class="zw">Avro相关知识本书不进行讲解，读者可查阅Avro官方网站提供的相关资料进行了解。Avro依赖Schema来实现数据结构定义，Avro的Schema主要由JSON对象来表示。通过Avro工具（本书通过Maven插件）将Schema定义的数据结构编译为对应的Java对象。</p>

  <p class="zw">本小节依然基于向Kafka推送股票行情的实例，详细讲解如何应用Avro框架自定义序列化和反序列化类，并配置自定义序列化与反序列化类实现向Kafka发送消息以及消费消息。</p>

  <h4 class="sigil_not_in_toc1">1．自定义序列化类</h4>

  <p class="zw">首先在本章对应工程的pom.xml文件中增加Avro依赖包配置，增加以下Maven依赖：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">&lt;dependency&gt;
  &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;
  &lt;artifactId&gt;avro&lt;/artifactId&gt;
  &lt;version&gt;1.8.1&lt;/version&gt;
&lt;/dependency&gt;</code></pre>

  <p class="zw">同时增加Maven编译Avro Schema的插件配置：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">&lt;build&gt;
  &lt;plugins&gt;
    &lt;plugin&gt;
      &lt;groupId&gt;org.apache.avro&lt;/groupId&gt;
      &lt;artifactId&gt;avro-maven-plugin&lt;/artifactId&gt;
      &lt;version&gt;1.8.1&lt;/version&gt;
      &lt;executions&gt;
        &lt;execution&gt;
          &lt;phase&gt;generate-sources&lt;/phase&gt;
          &lt;goals&gt;
            &lt;goal&gt;schema&lt;/goal&gt;
          &lt;/goals&gt;
          &lt;configuration&gt;
            &lt;sourceDirectory&gt;${project.basedir}/src/main/resources/
            &lt;/sourceDirectory&gt;
            &lt;outputDirectory&gt;${project.basedir}/src/main/Java/&lt;/outputDirectory&gt;
          &lt;/configuration&gt;
        &lt;/execution&gt;
      &lt;/executions&gt;
    &lt;/plugin&gt;
    &lt;plugin&gt;
      &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
      &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
      &lt;configuration&gt;
        &lt;source&gt;1.8&lt;/source&gt;
        &lt;target&gt;1.8&lt;/target&gt;
      &lt;/configuration&gt;
    &lt;/plugin&gt;
  &lt;/plugins&gt;
&lt;/build&gt;</code></pre>

  <p class="zw">然后定义一个序列化类AvroSerializer，该序列化类实现org.apache.kafka.common.serialization. Serializer接口，同时该序列化类接收Avro Schema编译生成的Java类，这些Java类都继承org.apache.avro.specific.SpecificRecordBase类，并实现该接口的serialize()方法，在该方法中利用Avro框架进行序列化操作，将SpecificRecordBase类型的对象转为字节数组。自定义的AvroSerializer类如代码清单6-24所示。</p>

  <p class="calibre21"><strong class="calibre1">代码清单6-24　AvroSerializer类的具体实现代码</strong></p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">package com.kafka.action.chapter6.avro;

import Java.io.ByteArrayOutputStream;
import Java.io.IOException;
import Java.util.Map;

import org.apache.avro.io.BinaryEncoder;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.kafka.common.errors.SerializationException;
import org.apache.kafka.common.serialization.Serializer;

public class AvroSerializer&lt;T extends SpecificRecordBase&gt; implements Serializer&lt;T&gt; {
    @Override
    public void configure(Map&lt;String, ?&gt; configs, boolean isKey) {
    }

    /**
     * 实现序列化方法
     */
    @Override
    public byte[] serialize(String topic, T data) {
        if (null == data) {
            return null;
        }
        DatumWriter&lt;T&gt; writer = new SpecificDatumWriter&lt;&gt;(data.getSchema());
        ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
        BinaryEncoder binaryEncoder = EncoderFactory.get().directBinaryEncoder(
                outputStream, null);
        try {
            writer.write(data, binaryEncoder);
        } catch (IOException e) {
            throw new SerializationException(e.getMessage());
        }
        return outputStream.toByteArray();
    }

    @Override
    public void close() {
    }
},</code></pre>

  <p class="zw">由于本章实例消息体为股票行情信息，因此定义一个股票行情信息对应的Schema文件。在工程的src/main/resources目录下创建一个名为stock_quotation.avs的文件，在该文件中定义股票行情信息对应的Avro Schema。该文件的内容如代码清单6-25所示。</p>

  <p class="calibre21"><strong class="calibre1">代码清单6-25　stock_quotation.avs文件的具体内容</strong></p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">{
    "namespace": "com.kafka.action.chapter6.avro",
    "type": "record",
    "name": "AvroStockQuotation",
    "fields": [
        {"name": "stockCode", "type": "string"},
        {"name": "stockName", "type": "string"},
        {"name": "tradeTime", "type": "long"},
        {"name": "preClosePrice", "type": "float"},
        {"name": "openPrice", "type": "float"},
        {"name": "currentPrice", "type": "float"},
        {"name": "highPrice", "type": "float"},
        {"name": "lowPrice", "type": "float"}
    ]
}</code></pre>

  <p class="zw">本例Avro Schema文件各字段说明如表6-1所示。</p>

  <p class="biao_ti">表6-1　Avro Schema基本字段说明</p>

  <table border="1" width="90%" class="calibre11">
    <thead class="calibre12">
      <tr class="calibre13">
        <th class="calibre14">
          <p class="biao_tou_dan_yuan_ge">字段</p>
        </th>

        <th class="calibre14">
          <p class="biao_tou_dan_yuan_ge">描述</p>
        </th>
      </tr>
    </thead>

    <tbody class="calibre15">
      <tr class="calibre13">
        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">namespace</p>
        </td>

        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">指定Java对象的包名</p>
        </td>
      </tr>

      <tr class="calibre17">
        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">fields</p>
        </td>

        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">指定属性域，各属性包括属性名和属性类型，也可为属性指定默认值</p>
        </td>
      </tr>

      <tr class="calibre13">
        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">type</p>
        </td>

        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">用于指定类型，若是fileds的子节点则表示属性字段类型，否则指Schema编译后的Java对象类型，若是普通JavaBean对象则该字段对应值为record</p>
        </td>
      </tr>

      <tr class="calibre17">
        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">name</p>
        </td>

        <td class="calibre16">
          <p class="biao_tou_dan_yuan_ge">Java类名或者属性名</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="zw">然后通过Maven插件编译该工程。在Eclipse中点击该工程右键Run As，选中Maven install运行编译该工程，编译完后刷新工程，在com.kafka.action.chapter6.avro包下生成一个AvroStock Quotation.Java类，该类继承自org.apache.avro.specific.SpecificRecordBase实现了org.apache.avro. specific. SpecificRecord接口。</p>

  <p class="zw">最后定义一个生产者AvroQuotationProducer类，在实例化KafkaProducer时指定消息体的序列化类为自定义的AvroSerializer类，该生产者发送AvroStockQuotation类型的消息。核心代码如下：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG 
               AvroSerializer.class.getName());
producer = new KafkaProducer&lt;String, AvroStockQuotation&gt;(configs);</code></pre>

  <p class="zw">在AvroQuotationProducer类中定义一个发送消息的方法，该方法的具体实现如代码清单6-26所示。</p>

  <p class="calibre21"><strong class="calibre1">代码清单6-26　AvroQuotationProducer发送消息方法的具体实现</strong></p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">public static void sendMsg(TopicEnum topic, AvroStockQuotation message) {
    if (null == message) {
        return;
    }
    if (StringUtils.equals(topic.getDataType().getClass().getName(),message. 
    getClass().getName())) {
        ProducerRecord&lt;String, AvroStockQuotation&gt; record = new ProducerRecord 
        &lt;String, AvroStockQuotation&gt;(topic.getTopicName(), 
        (String) message.getStockCode(),message);
        producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metaData, Exception exception) {
                if (null != exception) {// 发送异常记录异常信息
                    LOG.error("Send message occurs exception.", exception);
                }
                if (null != metaData) {
                    LOG.info(String.format("offset:%s,partition:%s",
                                           metaData.offset(), metaData.partition()));
                }
            }

        });
    }
}</code></pre>

  <p class="zw">该方法入参TopicEnum是一个枚举类，用于定义主题与消息类型的对应关系。该枚举类的具体实现如代码清单6-27所示，关于定义该枚举类的原因将在自定义反序列化类时进行介绍。</p>

  <p class="calibre21"><strong class="calibre1">代码清单6-27　TopicEnum枚举类的具体实现</strong></p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">package com.kafka.action.chapter6.avro;

import org.apache.avro.specific.SpecificRecordBase;
import org.apache.commons.lang.StringUtils;
public enum TopicEnum {
    STOCK_QUOTATION_AVRO("stock-quotation-avro", new AvroStockQuotation());
    public String topicName;
    public SpecificRecordBase dataType;

    private TopicEnum(String topicName, SpecificRecordBase dataType) {
        this.topicName = topicName;
        this.dataType = dataType;
    }
    ……省略了属性的get和set方法……
    public static TopicEnum getEnum(String topicName) {
        if (StringUtils.isBlank(topicName)) {
            return null;
        }
        for (TopicEnum topic : values()) {
            if (StringUtils.equalsIgnoreCase(topic.getTopicName(), topicName)) {
                return topic;
            }
        }
        return null;
    }
}</code></pre>

  <p class="zw">在main方法中，调用该生产者发送100条模拟股票行情信息的消息。在运行该方法之前，首先创建一个名为stock-quotation-avro的主题，创建该主题命令如下：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">kafka-topics.sh --create --zookeeper server-1:2181,server-2:2181,server-3:2181 --replication-factor 1 --partitions 6 --topic stock-quotation-avro</code></pre>

  <p class="zw">启动该生产者，在Eclipse控制台显示的信息部分内容如图6-1所示。</p>

  <p class="tu"><img alt="" src="../images/00119.gif" class="calibre7"/></p>

  <p class="tu_ti">图6-1　使用Avro序列化消息的生产者执行结果</p>

  <h4 class="sigil_not_in_toc1">2．自定义反序列化类</h4>

  <p class="zw">由于生产者发送消息时自定义了消息的序列化方式，因此消费者消费消息时也需要以同样的方式反序列化消息。</p>

  <p class="zw">首先，定义一个使用Avro框架反序列化的类AvroDeserializer，该类实现了org.apache.kafka. common.serialization.Deserializer接口，重写deserialize(String topic, byte[] data)方法，在该方法中将消息字节数组转为具体的消息实体对象。</p>

  <p class="zw">在应用Avro框架实现反序列化时，要通过具体实例类型的Schema实例化DatumReader。由于AvroDeserializer定义为一个泛型，通过Java反射机制将字节码数组得到具体类型比较复杂，因此一种简单的实现方式是直接将主题与该主题对应消息类型关联起来，因此我们定义了一个枚举类型的TopicEnum。</p>

  <p class="zw">AvroDeserizlizer类的具体实现代码如代码清单6-28所示。</p>

  <p class="zw"><strong class="calibre1">代码清单6-28　AvroDeserizlizer类的具体实现代码</strong></p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">package com.kafka.action.chapter6.avro;

import Java.io.ByteArrayInputStream;
import Java.util.Map;

import org.apache.avro.io.BinaryDecoder;
import org.apache.avro.io.DatumReader;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.avro.specific.SpecificRecordBase;
import org.apache.kafka.common.serialization.Deserializer;

import com.sun.xml.internal.ws.encoding.soap.DeserializationException;

public class AvroDeserializer&lt;T extends SpecificRecordBase&gt; implements
Deserializer&lt;T&gt; {

    @Override
    public void configure(Map&lt;String, ?&gt; configs, boolean isKey) {

    }

    @Override
    public void close() {

    }

    @Override
    public T deserialize(String topic, byte[] data) {
        if (null == data) {
            return null;
        }
        try {
            // 根据主题名从TopicEnum中获取该主题对应的SpecificRecordBase类型的实体类
            SpecificRecordBase record = TopicEnum.getEnum(topic).getDataType();
            if (null == record) {
                return null;
            }
            // 得到schema实例化DatumReader
            DatumReader&lt;T&gt; userDatumReader = new SpecificDatumReader&lt;&gt;(
                    record.getSchema());
            BinaryDecoder binaryEncoder = DecoderFactory.get()
                    .directBinaryDecoder(new ByteArrayInputStream(data), null);
            return userDatumReader.read(null, binaryEncoder);
        } catch (Exception e) {
            throw new DeserializationException(e.getMessage());
        }
    }
}</code></pre>

  <p class="zw">然后，实现一个消费者，在消费者实例化时指定value.deserializer配置项的值为自定义的反序列化类，代码如下：</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
          AvroDeserializer.class.getName());</code></pre>

  <p class="zw">该消费者简单打印出每条消息的偏移量、分区及消息实体对象AvroStockQuotation的股票代码与股票名，具体实现代码如代码清单6-29所示。</p>

  <p class="calibre21"><strong class="calibre1">代码清单6-29　使用Avro框架反序列化的消费者的具体实现代码</strong></p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre10">package com.kafka.action.chapter6.consumer;

import Java.util.Collections;
import Java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.log4j.Logger;

import com.kafka.action.chapter6.avro.AvroDeserializer;
import com.kafka.action.chapter6.avro.AvroStockQuotation;
import com.kafka.action.chapter6.avro.TopicEnum;

public class AvroQuotationConsumer {

    private static final Logger LOG = Logger.getLogger(AvroQuotationConsumer.class);

    private static final String BROKER_LIST = "server-1:9092,server-2:9092, 
    server-3:9092";

    private static final String GROUP_ID = "avro-consumer";

    private static final Long TIME_OUT = 30L;

    private static KafkaConsumer&lt;String, AvroStockQuotation&gt; consumer = new KafkaConsumer 
    &lt;String, AvroStockQuotation&gt;(initConfig());

    private static Properties initConfig() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_LIST);
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                  StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
                  AvroDeserializer.class.getName());
        return props;
    }

    public void consume(String topicName) {
        try {
            if (null == consumer) {
                consumer = new KafkaConsumer&lt;String, AvroStockQuotation&gt;(initConfig());
            }
            while (true) {
                consumer.subscribe(Collections.singletonList(topicName));
                ConsumerRecords&lt;String, AvroStockQuotation&gt; records = 
                        consumer.poll(TIME_OUT);
                AvroStockQuotation quotation = null;
                if (null != records) {
                    for (ConsumerRecord&lt;String, AvroStockQuotation&gt; record : records) {
                        quotation = record.value();
                        LOG.info(String.format("offset:%s,partition:%s,key:%s,
                                               value[stockCode%s,stockName%s]",
                                               record.offset(), record.partition(), 
                                               record.key(), quotation.getStockCode(),
                        quotation.getStockName()));
                    }
                }
            }
        } catch (Exception e) {
            LOG.error("Consume data from Kafka occurs exception", e);
        } finally {
            consumer.close();
            consumer = null;
        }
    }

    public static void main(String[] args) {
        AvroQuotationConsumer consumer = new AvroQuotationConsumer();
        while (true) {
            consumer.consume(TopicEnum.STOCK_QUOTATION_AVRO.getTopicName());
        }
    }
}</code></pre>

  <p class="zw">运行该消费者，控制台输出信息如图6-2所示。</p>

  <p class="tu"><img alt="" src="../images/00120.jpeg" class="calibre7"/></p>

  <p class="tu_ti">图6-2　使用Avro反序列化的消费者执行结果</p>

  

  </div>

  
  <div class="calibreToc">
    <h2><a href="../../jhnii3.html"> Table of contents</a></h2>
     <div>
  <ul>
    <li>
      <a href="part0001.html#UGI0-b6aea6b975744e46b4d1346849966264">版权信息</a>
    </li>
    <li>
      <a href="part0002.html#1T140-b6aea6b975744e46b4d1346849966264">内容提要</a>
    </li>
    <li>
      <a href="part0003_split_000.html#2RHM0-b6aea6b975744e46b4d1346849966264">前言</a>
    </li>
    <li>
      <a href="part0004_split_000.html#3Q280-b6aea6b975744e46b4d1346849966264">第1章 Kafka简介</a>
      <ul>
        <li>
          <a href="part0004_split_001.html">1.1 Kafka背景</a>
        </li>
        <li>
          <a href="part0004_split_002.html">1.2 Kafka基本结构</a>
        </li>
        <li>
          <a href="part0004_split_003.html">1.3 Kafka基本概念</a>
        </li>
        <li>
          <a href="part0004_split_004.html">1.4 Kafka设计概述</a>
          <ul>
            <li>
              <a href="part0004_split_004.html#nav_point_13">1.4.1 Kafka设计动机</a>
            </li>
            <li>
              <a href="part0004_split_004.html#nav_point_14">1.4.2 Kafka特性</a>
            </li>
            <li>
              <a href="part0004_split_004.html#nav_point_15">1.4.3 Kafka应用场景</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0004_split_005.html">1.5 本书导读</a>
        </li>
        <li>
          <a href="part0004_split_006.html">1.6 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0005_split_000.html#4OIQ0-b6aea6b975744e46b4d1346849966264">第2章 Kafka安装配置</a>
      <ul>
        <li>
          <a href="part0005_split_001.html">2.1 基础环境配置</a>
          <ul>
            <li>
              <a href="part0005_split_001.html#nav_point_20">2.1.1 JDK安装配置</a>
            </li>
            <li>
              <a href="part0005_split_001.html#nav_point_21">2.1.2 SSH安装配置</a>
            </li>
            <li>
              <a href="part0005_split_001.html#nav_point_22">2.1.3 ZooKeeper环境</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0005_split_002.html">2.2 Kafka单机环境部署</a>
          <ul>
            <li>
              <a href="part0005_split_002.html#nav_point_24">2.2.1 Windows环境安装Kafka</a>
            </li>
            <li>
              <a href="part0005_split_002.html#nav_point_25">2.2.2 Linux环境安装Kafka</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0005_split_003.html">2.3 Kafka伪分布式环境部署</a>
        </li>
        <li>
          <a href="part0005_split_004.html">2.4 Kafka集群环境部署</a>
        </li>
        <li>
          <a href="part0005_split_005.html">2.5 Kafka Manager安装</a>
        </li>
        <li>
          <a href="part0005_split_006.html">2.6 Kafka源码编译</a>
          <ul>
            <li>
              <a href="part0005_split_006.html#nav_point_30">2.6.1 Scala安装配置</a>
            </li>
            <li>
              <a href="part0005_split_006.html#nav_point_31">2.6.2 Gradle安装配置</a>
            </li>
            <li>
              <a href="part0005_split_006.html#nav_point_32">2.6.3 Kafka源码编译</a>
            </li>
            <li>
              <a href="part0005_split_006.html#nav_point_33">2.6.4 Kafka导入Eclipse</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0005_split_007.html">2.7 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0006_split_000.html#5N3C0-b6aea6b975744e46b4d1346849966264">第3章 Kafka核心组件</a>
      <ul>
        <li>
          <a href="part0006_split_001.html">3.1 延迟操作组件</a>
          <ul>
            <li>
              <a href="part0006_split_001.html#nav_point_37">3.1.1 DelayedOperation</a>
            </li>
            <li>
              <a href="part0006_split_001.html#nav_point_38">3.1.2 DelayedOperationPurgatory</a>
            </li>
            <li>
              <a href="part0006_split_001.html#nav_point_39">3.1.3 DelayedProduce</a>
            </li>
            <li>
              <a href="part0006_split_001.html#nav_point_40">3.1.4 DelayedFetch</a>
            </li>
            <li>
              <a href="part0006_split_001.html#nav_point_41">3.1.5 DelayedJoin</a>
            </li>
            <li>
              <a href="part0006_split_001.html#nav_point_42">3.1.6 DelayedHeartbeat</a>
            </li>
            <li>
              <a href="part0006_split_001.html#nav_point_43">3.1.7 DelayedCreateTopics</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0006_split_002.html">3.2 控制器</a>
          <ul>
            <li>
              <a href="part0006_split_002.html#nav_point_45">3.2.1 控制器初始化</a>
            </li>
            <li>
              <a href="part0006_split_002.html#nav_point_46">3.2.2 控制器选举过程</a>
            </li>
            <li>
              <a href="part0006_split_002.html#nav_point_47">3.2.3 故障转移</a>
            </li>
            <li>
              <a href="part0006_split_002.html#nav_point_48">3.2.4 代理上线与下线</a>
            </li>
            <li>
              <a href="part0006_split_002.html#nav_point_49">3.2.5 主题管理</a>
            </li>
            <li>
              <a href="part0006_split_002.html#nav_point_50">3.2.6 分区管理</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0006_split_003.html">3.3 协调器</a>
          <ul>
            <li>
              <a href="part0006_split_003.html#nav_point_52">3.3.1 消费者协调器</a>
            </li>
            <li>
              <a href="part0006_split_003.html#nav_point_53">3.3.2 组协调器</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0006_split_004.html">3.4 网络通信服务</a>
          <ul>
            <li>
              <a href="part0006_split_004.html#nav_point_55">3.4.1 Acceptor</a>
            </li>
            <li>
              <a href="part0006_split_004.html#nav_point_56">3.4.2 Processor</a>
            </li>
            <li>
              <a href="part0006_split_004.html#nav_point_57">3.4.3 RequestChannel</a>
            </li>
            <li>
              <a href="part0006_split_004.html#nav_point_58">3.4.4 SocketServer启动过程</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0006_split_005.html">3.5 日志管理器</a>
          <ul>
            <li>
              <a href="part0006_split_005.html#nav_point_60">3.5.1 Kafka日志结构</a>
            </li>
            <li>
              <a href="part0006_split_005.html#nav_point_61">3.5.2 日志管理器启动过程</a>
            </li>
            <li>
              <a href="part0006_split_005.html#nav_point_62">3.5.3 日志加载及恢复</a>
            </li>
            <li>
              <a href="part0006_split_005.html#nav_point_63">3.5.4 日志清理</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0006_split_006.html">3.6 副本管理器</a>
          <ul>
            <li>
              <a href="part0006_split_006.html#nav_point_65">3.6.1 分区</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_66">3.6.2 副本</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_67">3.6.3 副本管理器启动过程</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_68">3.6.4 副本过期检查</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_69">3.6.5 追加消息</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_70">3.6.6 拉取消息</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_71">3.6.7 副本同步过程</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_72">3.6.8 副本角色转换</a>
            </li>
            <li>
              <a href="part0006_split_006.html#nav_point_73">3.6.9 关闭副本</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0006_split_007.html">3.7 Handler</a>
        </li>
        <li>
          <a href="part0006_split_008.html">3.8 动态配置管理器</a>
        </li>
        <li>
          <a href="part0006_split_009.html">3.9 代理健康检测</a>
        </li>
        <li>
          <a href="part0006_split_010.html">3.10 Kafka内部监控</a>
        </li>
        <li>
          <a href="part0006_split_011.html">3.11 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0007_split_000.html#6LJU0-b6aea6b975744e46b4d1346849966264">第4章 Kafka核心流程分析</a>
      <ul>
        <li>
          <a href="part0007_split_001.html">4.1 KafkaServer启动流程分析</a>
        </li>
        <li>
          <a href="part0007_split_002.html">4.2 创建主题流程分析</a>
          <ul>
            <li>
              <a href="part0007_split_002.html#nav_point_82">4.2.1 客户端创建主题</a>
            </li>
            <li>
              <a href="part0007_split_002.html#nav_point_83">4.2.2 分区副本分配</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0007_split_003.html">4.3 生产者</a>
          <ul>
            <li>
              <a href="part0007_split_003.html#nav_point_85">4.3.1 Eclipse运行生产者源码</a>
            </li>
            <li>
              <a href="part0007_split_003.html#nav_point_86">4.3.2 生产者重要配置说明</a>
            </li>
            <li>
              <a href="part0007_split_003.html#nav_point_87">4.3.3 OldProducer执行流程</a>
            </li>
            <li>
              <a href="part0007_split_003.html#nav_point_88">4.3.4 KafkaProducer实现原理</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0007_split_004.html">4.4 消费者</a>
          <ul>
            <li>
              <a href="part0007_split_004.html#nav_point_90">4.4.1 旧版消费者</a>
            </li>
            <li>
              <a href="part0007_split_004.html#nav_point_91">4.4.2 KafkaConsumer初始化</a>
            </li>
            <li>
              <a href="part0007_split_004.html#nav_point_92">4.4.3 消费订阅</a>
            </li>
            <li>
              <a href="part0007_split_004.html#nav_point_93">4.4.4 消费消息</a>
            </li>
            <li>
              <a href="part0007_split_004.html#nav_point_94">4.4.5 消费偏移量提交</a>
            </li>
            <li>
              <a href="part0007_split_004.html#nav_point_95">4.4.6 心跳探测</a>
            </li>
            <li>
              <a href="part0007_split_004.html#nav_point_96">4.4.7 分区数与消费者线程的关系</a>
            </li>
            <li>
              <a href="part0007_split_004.html#nav_point_97">4.4.8 消费者平衡过程</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0007_split_005.html">4.5 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0008_split_000.html#7K4G0-b6aea6b975744e46b4d1346849966264">第5章 Kafka基本操作实战</a>
      <ul>
        <li>
          <a href="part0008_split_001.html">5.1 KafkaServer管理</a>
          <ul>
            <li>
              <a href="part0008_split_001.html#nav_point_101">5.1.1 启动Kafka单个节点</a>
            </li>
            <li>
              <a href="part0008_split_001.html#nav_point_102">5.1.2 启动Kafka集群</a>
            </li>
            <li>
              <a href="part0008_split_001.html#nav_point_103">5.1.3 关闭Kafka单个节点</a>
            </li>
            <li>
              <a href="part0008_split_001.html#nav_point_104">5.1.4 关闭Kafka集群</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_002.html">5.2 主题管理</a>
          <ul>
            <li>
              <a href="part0008_split_002.html#nav_point_106">5.2.1 创建主题</a>
            </li>
            <li>
              <a href="part0008_split_002.html#nav_point_107">5.2.2 删除主题</a>
            </li>
            <li>
              <a href="part0008_split_002.html#nav_point_108">5.2.3 查看主题</a>
            </li>
            <li>
              <a href="part0008_split_002.html#nav_point_109">5.2.4 修改主题</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_003.html">5.3 生产者基本操作</a>
          <ul>
            <li>
              <a href="part0008_split_003.html#nav_point_111">5.3.1 启动生产者</a>
            </li>
            <li>
              <a href="part0008_split_003.html#nav_point_112">5.3.2 创建主题</a>
            </li>
            <li>
              <a href="part0008_split_003.html#nav_point_113">5.3.3 查看消息</a>
            </li>
            <li>
              <a href="part0008_split_003.html#nav_point_114">5.3.4 生产者性能测试工具</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_004.html">5.4 消费者基本操作</a>
          <ul>
            <li>
              <a href="part0008_split_004.html#nav_point_116">5.4.1 消费消息</a>
            </li>
            <li>
              <a href="part0008_split_004.html#nav_point_117">5.4.2 单播与多播</a>
            </li>
            <li>
              <a href="part0008_split_004.html#nav_point_118">5.4.3 查看消费偏移量</a>
            </li>
            <li>
              <a href="part0008_split_004.html#nav_point_119">5.4.4 消费者性能测试工具</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_005.html">5.5 配置管理</a>
          <ul>
            <li>
              <a href="part0008_split_005.html#nav_point_121">5.5.1 主题级别配置</a>
            </li>
            <li>
              <a href="part0008_split_005.html#nav_point_122">5.5.2 代理级别设置</a>
            </li>
            <li>
              <a href="part0008_split_005.html#nav_point_123">5.5.3 客户端/用户级别配置</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_006.html">5.6 分区操作</a>
          <ul>
            <li>
              <a href="part0008_split_006.html#nav_point_125">5.6.1 分区Leader平衡</a>
            </li>
            <li>
              <a href="part0008_split_006.html#nav_point_126">5.6.2 分区迁移</a>
            </li>
            <li>
              <a href="part0008_split_006.html#nav_point_127">5.6.3 增加分区</a>
            </li>
            <li>
              <a href="part0008_split_006.html#nav_point_128">5.6.4 增加副本</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_007.html">5.7 连接器基本操作</a>
          <ul>
            <li>
              <a href="part0008_split_007.html#nav_point_130">5.7.1 独立模式</a>
            </li>
            <li>
              <a href="part0008_split_007.html#nav_point_131">5.7.2 REST风格API应用</a>
            </li>
            <li>
              <a href="part0008_split_007.html#nav_point_132">5.7.3 分布式模式</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_008.html">5.8 Kafka Manager应用</a>
        </li>
        <li>
          <a href="part0008_split_009.html">5.9 Kafka安全机制</a>
          <ul>
            <li>
              <a href="part0008_split_009.html#nav_point_135">5.9.1 利用SASL/PLAIN进行身份认证</a>
            </li>
            <li>
              <a href="part0008_split_009.html#nav_point_136">5.9.2 权限控制</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0008_split_010.html">5.10 镜像操作</a>
        </li>
        <li>
          <a href="part0008_split_011.html">5.11 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0009_split_000.html#8IL20-b6aea6b975744e46b4d1346849966264">第6章 Kafka API编程实战</a>
      <ul>
        <li>
          <a href="part0009_split_001.html">6.1 主题管理</a>
          <ul>
            <li>
              <a href="part0009_split_001.html#nav_point_141">6.1.1 创建主题</a>
            </li>
            <li>
              <a href="part0009_split_001.html#nav_point_142">6.1.2 修改主题级别配置</a>
            </li>
            <li>
              <a href="part0009_split_001.html#nav_point_143">6.1.3 增加分区</a>
            </li>
            <li>
              <a href="part0009_split_001.html#nav_point_144">6.1.4 分区副本重分配</a>
            </li>
            <li>
              <a href="part0009_split_001.html#nav_point_145">6.1.5 删除主题</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0009_split_002.html">6.2 生产者API应用</a>
          <ul>
            <li>
              <a href="part0009_split_002.html#nav_point_147">6.2.1 单线程生产者</a>
            </li>
            <li>
              <a href="part0009_split_002.html#nav_point_148">6.2.2 多线程生产者</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0009_split_003.html">6.3 消费者API应用</a>
          <ul>
            <li>
              <a href="part0009_split_003.html#nav_point_150">6.3.1 旧版消费者API应用</a>
            </li>
            <li>
              <a href="part0009_split_003.html#nav_point_151">6.3.2 新版消费者API应用</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0009_split_004.html">6.4 自定义组件实现</a>
          <ul>
            <li>
              <a href="part0009_split_004.html#nav_point_153">6.4.1 分区器</a>
            </li>
            <li>
              <a href="part0009_split_004.html#nav_point_154">6.4.2 序列化与反序列化</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0009_split_005.html">6.5 Spring与Kafka整合应用</a>
          <ul>
            <li>
              <a href="part0009_split_005.html#nav_point_156">6.5.1 生产者</a>
            </li>
            <li>
              <a href="part0009_split_005.html#nav_point_157">6.5.2 消费者</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0009_split_006.html">6.6 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0010_split_000.html#9H5K0-b6aea6b975744e46b4d1346849966264">第7章 Kafka Streams</a>
      <ul>
        <li>
          <a href="part0010_split_001.html">7.1 Kafka Streams简介</a>
        </li>
        <li>
          <a href="part0010_split_002.html">7.2 Kafka Streams基本概念</a>
          <ul>
            <li>
              <a href="part0010_split_002.html#nav_point_162">7.2.1 流</a>
            </li>
            <li>
              <a href="part0010_split_002.html#nav_point_163">7.2.2 流处理器</a>
            </li>
            <li>
              <a href="part0010_split_002.html#nav_point_164">7.2.3 处理器拓扑</a>
            </li>
            <li>
              <a href="part0010_split_002.html#nav_point_165">7.2.4 时间</a>
            </li>
            <li>
              <a href="part0010_split_002.html#nav_point_166">7.2.5 状态</a>
            </li>
            <li>
              <a href="part0010_split_002.html#nav_point_167">7.2.6 KStream和KTable</a>
            </li>
            <li>
              <a href="part0010_split_002.html#nav_point_168">7.2.7 窗口</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0010_split_003.html">7.3 Kafka Streams API介绍</a>
          <ul>
            <li>
              <a href="part0010_split_003.html#nav_point_170">7.3.1 KStream与KTable</a>
            </li>
            <li>
              <a href="part0010_split_003.html#nav_point_171">7.3.2 窗口操作</a>
            </li>
            <li>
              <a href="part0010_split_003.html#nav_point_172">7.3.3 连接操作</a>
            </li>
            <li>
              <a href="part0010_split_003.html#nav_point_173">7.3.4 变换操作</a>
            </li>
            <li>
              <a href="part0010_split_003.html#nav_point_174">7.3.5 聚合操作</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0010_split_004.html">7.4 接口恶意访问自动检测</a>
          <ul>
            <li>
              <a href="part0010_split_004.html#nav_point_176">7.4.1 应用描述</a>
            </li>
            <li>
              <a href="part0010_split_004.html#nav_point_177">7.4.2 具体实现</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0010_split_005.html">7.5 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0011_split_000.html#AFM60-b6aea6b975744e46b4d1346849966264">第8章 Kafka数据采集应用</a>
      <ul>
        <li>
          <a href="part0011_split_001.html">8.1 Log4j集成Kafka应用</a>
          <ul>
            <li>
              <a href="part0011_split_001.html#nav_point_181">8.1.1 应用描述</a>
            </li>
            <li>
              <a href="part0011_split_001.html#nav_point_182">8.1.2 具体实现</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0011_split_002.html">8.2 Kafka与Flume整合应用</a>
          <ul>
            <li>
              <a href="part0011_split_002.html#nav_point_184">8.2.1 Flume简介</a>
            </li>
            <li>
              <a href="part0011_split_002.html#nav_point_185">8.2.2 Flume与Kafka比较</a>
            </li>
            <li>
              <a href="part0011_split_002.html#nav_point_186">8.2.3 Flume的安装配置</a>
            </li>
            <li>
              <a href="part0011_split_002.html#nav_point_187">8.2.4 Flume采集日志写入Kafka</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0011_split_003.html">8.3 Kafka与Flume和HDFS整合应用</a>
          <ul>
            <li>
              <a href="part0011_split_003.html#nav_point_189">8.3.1 Hadoop安装配置</a>
            </li>
            <li>
              <a href="part0011_split_003.html#nav_point_190">8.3.2 Flume采集Kafka消息写入HDFS</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0011_split_004.html">8.4 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0012_split_000.html#BE6O0-b6aea6b975744e46b4d1346849966264">第9章 Kafka与ELK整合应用</a>
      <ul>
        <li>
          <a href="part0012_split_001.html">9.1 ELK环境搭建</a>
          <ul>
            <li>
              <a href="part0012_split_001.html#nav_point_194">9.1.1 Elasticsearch安装配置</a>
            </li>
            <li>
              <a href="part0012_split_001.html#nav_point_195">9.1.2 Logstash安装配置</a>
            </li>
            <li>
              <a href="part0012_split_001.html#nav_point_196">9.1.3 Kibana安装配置</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0012_split_002.html">9.2 Kafka与Logstash整合</a>
          <ul>
            <li>
              <a href="part0012_split_002.html#nav_point_198">9.2.1 Logstash收集日志到Kafka</a>
            </li>
            <li>
              <a href="part0012_split_002.html#nav_point_199">9.2.2 Logstash从Kafka消费日志</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0012_split_003.html">9.3 日志采集分析系统</a>
          <ul>
            <li>
              <a href="part0012_split_003.html#nav_point_201">9.3.1 Flume采集日志配置</a>
            </li>
            <li>
              <a href="part0012_split_003.html#nav_point_202">9.3.2 Logstash拉取日志配置</a>
            </li>
            <li>
              <a href="part0012_split_003.html#nav_point_203">9.3.3 Kibana日志展示</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0012_split_004.html">9.4 服务器性能监控系统</a>
          <ul>
            <li>
              <a href="part0012_split_004.html#nav_point_205">9.4.1 Metricbeat安装</a>
            </li>
            <li>
              <a href="part0012_split_004.html#nav_point_206">9.4.2 采集信息存储到Elasticsearch</a>
            </li>
            <li>
              <a href="part0012_split_004.html#nav_point_207">9.4.3 加载beats-dashboards</a>
            </li>
            <li>
              <a href="part0012_split_004.html#nav_point_208">9.4.4 服务器性能监控系统具体实现</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0012_split_005.html">9.5 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0013_split_000.html#CCNA0-b6aea6b975744e46b4d1346849966264">第10章 Kafka与Spark整合应用</a>
      <ul>
        <li>
          <a href="part0013_split_001.html">10.1 Spark简介</a>
        </li>
        <li>
          <a href="part0013_split_002.html">10.2 Spark基本操作</a>
          <ul>
            <li>
              <a href="part0013_split_002.html#nav_point_213">10.2.1 Spark安装</a>
            </li>
            <li>
              <a href="part0013_split_002.html#nav_point_214">10.2.2 Spark shell应用</a>
            </li>
            <li>
              <a href="part0013_split_002.html#nav_point_215">10.2.3 spark-submit提交作业</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0013_split_003.html">10.3 Spark在智能投顾领域应用</a>
          <ul>
            <li>
              <a href="part0013_split_003.html#nav_point_217">10.3.1 应用描述</a>
            </li>
            <li>
              <a href="part0013_split_003.html#nav_point_218">10.3.2 具体实现</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0013_split_004.html">10.4 热搜词统计</a>
          <ul>
            <li>
              <a href="part0013_split_004.html#nav_point_220">10.4.1 应用描述</a>
            </li>
            <li>
              <a href="part0013_split_004.html#nav_point_221">10.4.2 具体实现</a>
            </li>
          </ul>
        </li>
        <li>
          <a href="part0013_split_005.html">10.5 小结</a>
        </li>
      </ul>
    </li>
    <li>
      <a href="part0014_split_000.html#DB7S0-b6aea6b975744e46b4d1346849966264">欢迎来到异步社区！</a>
    </li>
  </ul>
</div>


  </div>
  

  <div class="calibreEbNav">
    
      <a href="part0009_split_003.html" class="calibreAPrev">previous page</a>
    

    <a href="../../jhnii3.html" class="calibreAHome"> start</a>

    
      <a href="part0009_split_005.html" class="calibreANext"> next page</a>
    
  </div>

</div>

</body>
</html>
